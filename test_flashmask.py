#!/usr/bin/env python3
"""
æµ‹è¯•FlashMaskPyTorchå®ç°
éªŒè¯åŠ¨æ€æ©ç åŠŸèƒ½å’Œæ€§èƒ½
"""

import torch
import torch.nn.functional as F
import time
import numpy as np
from typing import List
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append('/root/ml-fastvlm')
from llava.model.language_model.flashmask_pytorch import FlashMaskPyTorch

def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("=" * 50)
    print("æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, num_heads, head_dim = 2, 128, 8, 64
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºè¾“å…¥
    q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    
    # åˆ›å»ºFlashMaskå®ä¾‹
    flashmask = FlashMaskPyTorch(use_flash_attn=True)
    
    # æµ‹è¯•åŠ¨æ€æ©ç ç”Ÿæˆ
    image_range_list = [
        [[10, 20], [30, 40]],  # batch 0: ä¸¤ä¸ªå›¾åƒèŒƒå›´
        [[50, 60]]              # batch 1: ä¸€ä¸ªå›¾åƒèŒƒå›´
    ]
    
    try:
        # æµ‹è¯•ç¨€ç–æ©ç è¡¨ç¤º
        mask_indices, mask_values = flashmask.create_sparse_mask_representation(
            batch_size, num_heads, seq_len, device, q.dtype, 
            image_range_list, grid_size=12
        )
        
        print(f"âœ… ç¨€ç–æ©ç ç”ŸæˆæˆåŠŸ")
        print(f"   - æ©ç ç´¢å¼•å½¢çŠ¶: {mask_indices.shape}")
        print(f"   - æ©ç å€¼å½¢çŠ¶: {mask_values.shape}")
        
        # æµ‹è¯•æ³¨æ„åŠ›è®¡ç®—
        output = flashmask.forward(q, k, v, image_range_list, grid_size=12)
        
        print(f"âœ… æ³¨æ„åŠ›è®¡ç®—æˆåŠŸ")
        print(f"   - è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   - è¾“å‡ºæ•°æ®ç±»å‹: {output.dtype}")
        print(f"   - è¾“å‡ºè®¾å¤‡: {output.device}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_mask_correctness():
    """æµ‹è¯•æ©ç æ­£ç¡®æ€§"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•2: æ©ç æ­£ç¡®æ€§æµ‹è¯•")
    print("=" * 50)
    
    # åˆ›å»ºç®€å•æµ‹è¯•æ•°æ®
    batch_size, seq_len, num_heads, head_dim = 1, 16, 2, 8
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºè¾“å…¥
    q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    
    # åˆ›å»ºFlashMaskå®ä¾‹
    flashmask = FlashMaskPyTorch(use_flash_attn=False)  # ä½¿ç”¨æ‰‹åŠ¨å®ç°ä¾¿äºè°ƒè¯•
    
    # æµ‹è¯•ä¸åŒçš„æ©ç æ¨¡å¼
    test_cases = [
        {
            "name": "å› æœæ©ç ",
            "image_range_list": [[]],
            "expected_behavior": "åº”è¯¥é˜»æ­¢æœªæ¥tokençš„æ³¨æ„åŠ›"
        },
        {
            "name": "åŠ¨æ€æ©ç ",
            "image_range_list": [[[4, 8]]],
            "expected_behavior": "åº”è¯¥å…è®¸ç‰¹å®šèŒƒå›´çš„æ³¨æ„åŠ›"
        },
        {
            "name": "å¤šèŒƒå›´æ©ç ",
            "image_range_list": [[[2, 4], [8, 12]]],
            "expected_behavior": "åº”è¯¥å…è®¸å¤šä¸ªèŒƒå›´çš„æ³¨æ„åŠ›"
        }
    ]
    
    for i, test_case in enumerate(test_cases):
        try:
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case['name']}")
            print(f"æœŸæœ›è¡Œä¸º: {test_case['expected_behavior']}")
            
            output = flashmask.forward(
                q, k, v, 
                test_case['image_range_list'], 
                grid_size=4
            )
            
            print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"âœ… è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•ç”¨ä¾‹ {i+1} å¤±è´¥: {e}")

def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•3: æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")
        return
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, num_heads, head_dim = 4, 512, 16, 64
    device = torch.device('cuda')
    
    # åˆ›å»ºè¾“å…¥
    q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    
    # åˆ›å»ºå›¾åƒèŒƒå›´åˆ—è¡¨
    image_range_list = [
        [[i*10, (i+1)*10] for i in range(5)] for _ in range(batch_size)
    ]
    
    # æµ‹è¯•FlashMaskæ€§èƒ½
    flashmask = FlashMaskPyTorch(use_flash_attn=True)
    
    # é¢„çƒ­
    for _ in range(5):
        _ = flashmask.forward(q, k, v, image_range_list, grid_size=12)
    
    torch.cuda.synchronize()
    
    # æµ‹è¯•FlashMask
    start_time = time.time()
    for _ in range(10):
        output_flashmask = flashmask.forward(q, k, v, image_range_list, grid_size=12)
    torch.cuda.synchronize()
    flashmask_time = time.time() - start_time
    
    # æµ‹è¯•æ ‡å‡†æ³¨æ„åŠ›
    start_time = time.time()
    for _ in range(10):
        # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
        q_reshaped = q.transpose(1, 2)  # (batch, heads, seq, dim)
        k_reshaped = k.transpose(1, 2)
        v_reshaped = v.transpose(1, 2)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(q_reshaped, k_reshaped.transpose(-2, -1)) / (head_dim ** 0.5)
        
        # åº”ç”¨å› æœæ©ç 
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
        scores = scores.masked_fill(causal_mask == 0, float('-inf'))
        
        # åº”ç”¨softmax
        attn_weights = F.softmax(scores, dim=-1)
        
        # è®¡ç®—è¾“å‡º
        output_standard = torch.matmul(attn_weights, v_reshaped)
        output_standard = output_standard.transpose(1, 2)
    
    torch.cuda.synchronize()
    standard_time = time.time() - start_time
    
    print(f"âœ… FlashMaskæ—¶é—´: {flashmask_time:.4f}s")
    print(f"âœ… æ ‡å‡†æ³¨æ„åŠ›æ—¶é—´: {standard_time:.4f}s")
    print(f"âœ… åŠ é€Ÿæ¯”: {standard_time/flashmask_time:.2f}x")
    
    # éªŒè¯è¾“å‡ºä¸€è‡´æ€§
    try:
        # ä½¿ç”¨æ‰‹åŠ¨å®ç°è¿›è¡Œå¯¹æ¯”
        flashmask_manual = FlashMaskPyTorch(use_flash_attn=False)
        output_manual = flashmask_manual.forward(q, k, v, image_range_list, grid_size=12)
        
        # è®¡ç®—å·®å¼‚
        diff = torch.abs(output_flashmask - output_manual).max().item()
        print(f"âœ… FlashMask vs æ‰‹åŠ¨å®ç°å·®å¼‚: {diff:.6f}")
        
        if diff < 1e-3:
            print("âœ… è¾“å‡ºä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        else:
            print("âš ï¸  è¾“å‡ºå­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½éœ€è¦è°ƒè¯•")
            
    except Exception as e:
        print(f"âš ï¸  ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•4: å†…å­˜ä½¿ç”¨æµ‹è¯•")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return
    
    device = torch.device('cuda')
    
    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
    seq_lengths = [128, 256, 512, 1024]
    batch_size, num_heads, head_dim = 2, 8, 64
    
    print("åºåˆ—é•¿åº¦ | FlashMaskå†…å­˜ | æ ‡å‡†æ³¨æ„åŠ›å†…å­˜ | å†…å­˜èŠ‚çœ")
    print("-" * 60)
    
    for seq_len in seq_lengths:
        try:
            # åˆ›å»ºè¾“å…¥
            q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
            k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
            v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
            
            # åˆ›å»ºå›¾åƒèŒƒå›´åˆ—è¡¨
            image_range_list = [[[i*10, (i+1)*10] for i in range(seq_len//20)] for _ in range(batch_size)]
            
            # æµ‹è¯•FlashMaskå†…å­˜
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            flashmask = FlashMaskPyTorch(use_flash_attn=True)
            _ = flashmask.forward(q, k, v, image_range_list, grid_size=12)
            
            flashmask_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
            # æµ‹è¯•æ ‡å‡†æ³¨æ„åŠ›å†…å­˜
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
            q_reshaped = q.transpose(1, 2)
            k_reshaped = k.transpose(1, 2)
            v_reshaped = v.transpose(1, 2)
            
            scores = torch.matmul(q_reshaped, k_reshaped.transpose(-2, -1)) / (head_dim ** 0.5)
            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
            scores = scores.masked_fill(causal_mask == 0, float('-inf'))
            attn_weights = F.softmax(scores, dim=-1)
            _ = torch.matmul(attn_weights, v_reshaped)
            
            standard_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
            # è®¡ç®—å†…å­˜èŠ‚çœ
            memory_saving = (standard_memory - flashmask_memory) / standard_memory * 100
            
            print(f"{seq_len:8d} | {flashmask_memory:10.1f}MB | {standard_memory:12.1f}MB | {memory_saving:6.1f}%")
            
        except Exception as e:
            print(f"{seq_len:8d} | æµ‹è¯•å¤±è´¥: {e}")

def test_edge_cases():
    """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•5: è¾¹ç•Œæƒ…å†µæµ‹è¯•")
    print("=" * 50)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    flashmask = FlashMaskPyTorch(use_flash_attn=True)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "ç©ºå›¾åƒèŒƒå›´",
            "batch_size": 1,
            "seq_len": 32,
            "image_range_list": [[]],
            "expected": "åº”è¯¥æ­£å¸¸å·¥ä½œ"
        },
        {
            "name": "å•tokenåºåˆ—",
            "batch_size": 1,
            "seq_len": 1,
            "image_range_list": [[[0, 1]]],
            "expected": "åº”è¯¥æ­£å¸¸å·¥ä½œ"
        },
        {
            "name": "å¤§batch size",
            "batch_size": 16,
            "seq_len": 64,
            "image_range_list": [[[i*2, (i+1)*2] for i in range(10)] for _ in range(16)],
            "expected": "åº”è¯¥æ­£å¸¸å·¥ä½œ"
        }
    ]
    
    for i, test_case in enumerate(test_cases):
        try:
            print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}: {test_case['name']}")
            print(f"æœŸæœ›: {test_case['expected']}")
            
            # åˆ›å»ºè¾“å…¥
            q = torch.randn(test_case['batch_size'], test_case['seq_len'], 8, 64, device=device)
            k = torch.randn(test_case['batch_size'], test_case['seq_len'], 8, 64, device=device)
            v = torch.randn(test_case['batch_size'], test_case['seq_len'], 8, 64, device=device)
            
            # æµ‹è¯•
            output = flashmask.forward(q, k, v, test_case['image_range_list'], grid_size=12)
            
            print(f"âœ… æˆåŠŸ - è¾“å‡ºå½¢çŠ¶: {output.shape}")
            
        except Exception as e:
            print(f"âŒ å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("FlashMaskPyTorch æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.get_device_name()}")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        test_basic_functionality,
        test_mask_correctness,
        test_performance,
        test_memory_usage,
        test_edge_cases
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_func.__name__} å¼‚å¸¸: {e}")
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    print(f"é€šè¿‡æµ‹è¯•: {passed}/{total}")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼FlashMaskPyTorchå®ç°æ­£ç¡®")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
