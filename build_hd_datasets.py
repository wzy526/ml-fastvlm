import os
import json
import random
from PIL import Image
from tqdm import tqdm
from datasets import load_dataset

# ================= é…ç½®åŒº =================
SFT_DATA_DIR = "./sft_data"
TRAIN_SPLIT_DIR = os.path.join(SFT_DATA_DIR, "train_split")
BASE_LLAVA_JSON = os.path.join(SFT_DATA_DIR, "llava_v1_5_mix665k_filtered.json")
FINAL_OUTPUT_JSON = os.path.join(SFT_DATA_DIR, "llava_v1_5_mix665k_shuffled.json")

# ç¡®ä¿å­ç›®å½•å­˜åœ¨
for sub_dir in ["docvqa", "infovqa", "synthdog", "refcoco"]:
    os.makedirs(os.path.join(TRAIN_SPLIT_DIR, sub_dir), exist_ok=True)

# RefCOCO æç¤ºè¯æ¨¡æ¿
REFCOCO_PROMPTS = [
    "Provide the bounding box for the region that this text describes: <expr>",
    "Where is <expr> located in the image? Please output the bounding box.",
    "Find the region matching the description '<expr>' and give its coordinates."
]
# ==========================================

def normalize_bbox(coco_bbox, img_w, img_h):
    """å°† [x_min, y_min, w, h] è½¬æ¢ä¸º [x1, y1, x2, y2] çš„ 0~1 æµ®ç‚¹æ•°"""
    x_min, y_min, w, h = coco_bbox
    x1 = max(0.0, min(1.0, x_min / img_w))
    y1 = max(0.0, min(1.0, y_min / img_h))
    x2 = max(0.0, min(1.0, (x_min + w) / img_w))
    y2 = max(0.0, min(1.0, (y_min + h) / img_h))
    return f"[{x1:.3f}, {y1:.3f}, {x2:.3f}, {y2:.3f}]"

def process_hf_vqa_dataset(dataset_name, split, subset_name, save_dir_name, limit=None):
    """é€šç”¨çš„ Hugging Face VQA æ•°æ®é›†ä¸‹è½½ä¸è½¬æ¢é€»è¾‘"""
    print(f"Loading {dataset_name} ({subset_name})...")
    ds = load_dataset(dataset_name, subset_name, split=split)
    
    if limit:
        # éšæœºé™é‡‡æ · (åº”å¯¹ SynthDog è¿™ç±» 50ä¸‡æ¡çš„å·¨æ— éœ¸)
        indices = random.sample(range(len(ds)), limit)
        ds = ds.select(indices)
        
    augmented_data = []
    save_dir = os.path.join(TRAIN_SPLIT_DIR, save_dir_name)
    
    for idx, item in enumerate(tqdm(ds, desc=f"Processing {save_dir_name}")):
        img = item['image']
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶åå¹¶ä¿å­˜åŸå›¾
        img_filename = f"{save_dir_name}_{idx}.jpg"
        img_path = os.path.join(save_dir, img_filename)
        
        # å­˜å›¾åˆ°æœ¬åœ° train_split/ ç›®å½•
        if not os.path.exists(img_path):
            img.convert("RGB").save(img_path)
            
        # ç»„è£… LLaVA æ ¼å¼
        # æ³¨æ„ï¼šDocVQA å­—æ®µæ˜¯ 'question', 'answers'ï¼›SynthDog æ˜¯ 'text' (æ²¡é—®é¢˜ï¼Œæˆ‘ä»¬åŠ¨æ€é€‚é…)
        question = item.get('question', "What is written in this image?")
        answers = item.get('answers', [item.get('text', '')])
        answer = answers[0] if isinstance(answers, list) else answers
        
        llava_sample = {
            "id": f"{save_dir_name}_{idx}",
            "image": f"{save_dir_name}/{img_filename}",  # ç›¸å¯¹è·¯å¾„
            "conversations": [
                {"from": "human", "value": f"<image>\n{question}"},
                {"from": "gpt", "value": answer}
            ]
        }
        augmented_data.append(llava_sample)
        
    return augmented_data

def process_refcoco_from_hf():
    """ä» Hugging Face å¤„ç† RefCOCOï¼Œå¹¶æ˜ å°„åˆ° train2017"""
    print("Loading RefCOCO...")
    # è¿™é‡Œä½¿ç”¨ç¤¾åŒºå¤„ç†å¥½çš„æ‰å¹³åŒ– RefCOCO é›†åˆ
    ds = load_dataset("PaDT-MLLM/RefCOCO", split="train")
    augmented_data = []
    
    coco_train2017_dir = os.path.join(TRAIN_SPLIT_DIR, "coco", "train2017")
    
    for idx, item in enumerate(tqdm(ds, desc="Processing RefCOCO")):
        # 1. æå– ID å¹¶æ˜ å°„åˆ° train2017 æ–‡ä»¶å
        image_id = item['image_id']
        img_filename = f"{int(image_id):012d}.jpg"
        img_full_path = os.path.join(coco_train2017_dir, img_filename)
        
        # å› ä¸ºæå°‘æ•°(ä¸åˆ°4%)çš„ COCO2014 å›¾è¢«åˆ†åˆ°äº† val2017ï¼Œå¦‚æœåœ¨ train2017 æ‰¾ä¸åˆ°å°±è·³è¿‡
        if not os.path.exists(img_full_path):
            continue
            
        # 2. è¯»å–å›¾åƒå®½é«˜è¿›è¡Œå½’ä¸€åŒ–
        with Image.open(img_full_path) as img:
            img_w, img_h = img.size
            
        norm_bbox_str = normalize_bbox(item['bbox'], img_w, img_h)
        prompt = random.choice(REFCOCO_PROMPTS).replace("<expr>", item['sentences'][0])
        
        llava_sample = {
            "id": f"refcoco_{idx}",
            "image": f"coco/train2017/{img_filename}",
            "conversations": [
                {"from": "human", "value": f"<image>\n{prompt}"},
                {"from": "gpt", "value": norm_bbox_str}
            ]
        }
        augmented_data.append(llava_sample)
        
    return augmented_data

def main():
    random.seed(42)
    all_new_data = []
    
    # 1. ä¸‹è½½å¹¶å¤„ç† DocVQA (~39k)
    all_new_data.extend(process_hf_vqa_dataset("HuggingFaceM4/DocVQA", "train", "DocVQA", "docvqa"))
    
    # 2. ä¸‹è½½å¹¶å¤„ç† InfoVQA (~24k)
    all_new_data.extend(process_hf_vqa_dataset("HuggingFaceM4/infovqa", "train", "infovqa", "infovqa"))
    
    # 3. ä¸‹è½½å¹¶å¤„ç† SynthDoG (å®˜æ–¹æœ‰50ä¸‡ï¼Œè¿™é‡Œæˆ‘ä»¬éšæœºé™é‡‡æ ·ä¿ç•™ 20ä¸‡)
    all_new_data.extend(process_hf_vqa_dataset("naver-clova-ix/synthdog-en", "train", "synthdog-en", "synthdog", limit=200000))
    
    # 4. å¤„ç† RefCOCO (è‡ªåŠ¨æ˜ å°„ç°æœ‰ COCO å›¾ç‰‡)
    all_new_data.extend(process_refcoco_from_hf())
    
    print(f"\n[Stats] Generated {len(all_new_data)} augmented samples.")
    
    # 5. åˆå¹¶åŸºç¡€æ•°æ®
    print(f"Loading base dataset from {BASE_LLAVA_JSON}...")
    with open(BASE_LLAVA_JSON, 'r') as f:
        llava_base_data = json.load(f)
        
    final_merged_data = llava_base_data + all_new_data
    
    # 6. å…¨å±€æ‰“ä¹± (å¿…é¡»åšï¼)
    print("Shuffling final dataset...")
    random.shuffle(final_merged_data)
    
    # 7. ä¿å­˜ç»ˆæ JSON
    with open(FINAL_OUTPUT_JSON, 'w') as f:
        json.dump(final_merged_data, f)
        
    print(f"\nğŸ‰ ALL DONE! Saved {len(final_merged_data)} samples to {FINAL_OUTPUT_JSON}.")

if __name__ == "__main__":
    main()