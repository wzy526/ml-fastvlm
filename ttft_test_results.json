{
  "model_config": "LLaVA_FastViTHD_0.5B_Stage2",
  "vision_encoder": "FastViTHD",
  "llm": "Llama",
  "input_resolution": "1024x1024",
  "visual_tokens": 256,
  "model_size": "0.5B",
  "stage": 2,
  "model_path": "./checkpoints/llava-fastvithd_0.5b_stage2",
  "data_path": "/cluster/home/data/gqa/questions/val_balanced_questions.json",
  "total_samples": 1000,
  "accumulated_latency_ms": 103633.9375,
  "avg_ttft_ms": 103.6339375,
  "calculation_method": "dist.all_reduce(sum)",
  "world_size": 8,
  "timestamp": "2025-07-31 07:01:13"
}