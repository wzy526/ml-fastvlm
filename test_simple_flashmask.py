#!/usr/bin/env python3
"""
æµ‹è¯•ç®€å•çš„FlashMaskå®ç°
åŸºäºPyTorchåŸç”Ÿä¼˜åŒ–
"""

import torch
import time
import sys
import os

# æ·»åŠ è·¯å¾„
sys.path.append('/root/ml-fastvlm')
from llava.model.language_model.flashmask_simple import FlashMaskPyTorch, flashmask_attention

def test_simple_implementation():
    """æµ‹è¯•ç®€å•å®ç°"""
    print("=" * 50)
    print("æµ‹è¯•ç®€å•FlashMaskå®ç°")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, num_heads, head_dim = 2, 128, 8, 64
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºè¾“å…¥
    q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    
    # åˆ›å»ºå›¾åƒèŒƒå›´åˆ—è¡¨
    image_range_list = [
        [[10, 20], [30, 40]],  # batch 0
        [[50, 60]]              # batch 1
    ]
    
    print(f"âœ… è¾“å…¥å½¢çŠ¶: {q.shape}")
    print(f"âœ… è®¾å¤‡: {device}")
    
    # æµ‹è¯•å‡½æ•°è°ƒç”¨
    try:
        output = flashmask_attention(q, k, v, image_range_list, grid_size=12)
        print(f"âœ… å‡½æ•°è°ƒç”¨æˆåŠŸ")
        print(f"   - è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"   - è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
    except Exception as e:
        print(f"âŒ å‡½æ•°è°ƒç”¨å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•ç±»è°ƒç”¨
    try:
        flashmask = FlashMaskPyTorch(use_flash_attn=True)
        output_class = flashmask.forward(q, k, v, image_range_list, grid_size=12)
        print(f"âœ… ç±»è°ƒç”¨æˆåŠŸ")
        print(f"   - è¾“å‡ºå½¢çŠ¶: {output_class.shape}")
        
        # éªŒè¯ä¸€è‡´æ€§
        diff = torch.abs(output - output_class).max().item()
        print(f"âœ… ä¸€è‡´æ€§æ£€æŸ¥: {diff:.6f}")
        
    except Exception as e:
        print(f"âŒ ç±»è°ƒç”¨å¤±è´¥: {e}")
        return False
    
    return True

def test_performance():
    """æµ‹è¯•æ€§èƒ½"""
    print("\n" + "=" * 50)
    print("æ€§èƒ½æµ‹è¯•")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡æ€§èƒ½æµ‹è¯•")
        return
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, num_heads, head_dim = 4, 512, 16, 64
    device = torch.device('cuda')
    
    q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
    
    # åˆ›å»ºå›¾åƒèŒƒå›´åˆ—è¡¨
    image_range_list = [
        [[i*10, (i+1)*10] for i in range(5)] for _ in range(batch_size)
    ]
    
    # é¢„çƒ­
    for _ in range(5):
        _ = flashmask_attention(q, k, v, image_range_list, grid_size=12)
    
    torch.cuda.synchronize()
    
    # æµ‹è¯•FlashMask
    start_time = time.time()
    for _ in range(10):
        output_flashmask = flashmask_attention(q, k, v, image_range_list, grid_size=12)
    torch.cuda.synchronize()
    flashmask_time = time.time() - start_time
    
    # æµ‹è¯•æ ‡å‡†æ³¨æ„åŠ›
    start_time = time.time()
    for _ in range(10):
        # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
        q_reshaped = q.transpose(1, 2)  # (batch, heads, seq, dim)
        k_reshaped = k.transpose(1, 2)
        v_reshaped = v.transpose(1, 2)
        
        scores = torch.matmul(q_reshaped, k_reshaped.transpose(-2, -1)) / (head_dim ** 0.5)
        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
        scores = scores.masked_fill(causal_mask == 0, float('-inf'))
        attn_weights = F.softmax(scores, dim=-1)
        output_standard = torch.matmul(attn_weights, v_reshaped)
        output_standard = output_standard.transpose(1, 2)
    
    torch.cuda.synchronize()
    standard_time = time.time() - start_time
    
    print(f"âœ… FlashMaskæ—¶é—´: {flashmask_time:.4f}s")
    print(f"âœ… æ ‡å‡†æ³¨æ„åŠ›æ—¶é—´: {standard_time:.4f}s")
    print(f"âœ… åŠ é€Ÿæ¯”: {standard_time/flashmask_time:.2f}x")
    
    # éªŒè¯è¾“å‡ºä¸€è‡´æ€§
    try:
        diff = torch.abs(output_flashmask - output_standard).max().item()
        print(f"âœ… è¾“å‡ºå·®å¼‚: {diff:.6f}")
        
        if diff < 1e-3:
            print("âœ… è¾“å‡ºä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        else:
            print("âš ï¸  è¾“å‡ºå­˜åœ¨å·®å¼‚")
            
    except Exception as e:
        print(f"âš ï¸  ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")

def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    print("\n" + "=" * 50)
    print("å†…å­˜ä½¿ç”¨æµ‹è¯•")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return
    
    device = torch.device('cuda')
    
    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦
    seq_lengths = [128, 256, 512, 1024]
    batch_size, num_heads, head_dim = 2, 8, 64
    
    print("åºåˆ—é•¿åº¦ | FlashMaskå†…å­˜ | æ ‡å‡†æ³¨æ„åŠ›å†…å­˜ | å†…å­˜èŠ‚çœ")
    print("-" * 60)
    
    for seq_len in seq_lengths:
        try:
            # åˆ›å»ºè¾“å…¥
            q = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
            k = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
            v = torch.randn(batch_size, seq_len, num_heads, head_dim, device=device)
            
            # åˆ›å»ºå›¾åƒèŒƒå›´åˆ—è¡¨
            image_range_list = [[[i*10, (i+1)*10] for i in range(seq_len//20)] for _ in range(batch_size)]
            
            # æµ‹è¯•FlashMaskå†…å­˜
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            _ = flashmask_attention(q, k, v, image_range_list, grid_size=12)
            
            flashmask_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
            # æµ‹è¯•æ ‡å‡†æ³¨æ„åŠ›å†…å­˜
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
            
            # æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
            q_reshaped = q.transpose(1, 2)
            k_reshaped = k.transpose(1, 2)
            v_reshaped = v.transpose(1, 2)
            
            scores = torch.matmul(q_reshaped, k_reshaped.transpose(-2, -1)) / (head_dim ** 0.5)
            causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
            scores = scores.masked_fill(causal_mask == 0, float('-inf'))
            attn_weights = F.softmax(scores, dim=-1)
            _ = torch.matmul(attn_weights, v_reshaped)
            
            standard_memory = torch.cuda.max_memory_allocated() / 1024**2  # MB
            
            # è®¡ç®—å†…å­˜èŠ‚çœ
            memory_saving = (standard_memory - flashmask_memory) / standard_memory * 100
            
            print(f"{seq_len:8d} | {flashmask_memory:10.1f}MB | {standard_memory:12.1f}MB | {memory_saving:6.1f}%")
            
        except Exception as e:
            print(f"{seq_len:8d} | æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ç®€å•FlashMaskå®ç°æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥ç¯å¢ƒ
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.get_device_name()}")
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        test_simple_implementation,
        test_performance,
        test_memory_usage
    ]
    
    passed = 0
    total = len(tests)
    
    for test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"âŒ æµ‹è¯• {test_func.__name__} å¼‚å¸¸: {e}")
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    print(f"é€šè¿‡æµ‹è¯•: {passed}/{total}")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç®€å•FlashMaskå®ç°æ­£ç¡®")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
