CUDA_VISIBLE_DEVICES=0 python eval_gqa_official.py \
  --model-path /perception-hl/zhuofan.xia/vlm_exps/textdat/tdat-7b-l0d32-s12g8z3 \
  --data-path /perception-hl/zhuofan.xia/data/gqa/val_balanced_questions.json \
  --image-folder /perception-hl/zhuofan.xia/data/gqa/images \
  --output-dir ./gqa_debug \
  --conv-mode llava_v1 \
  --temperature 0 \
  --max-new-tokens 16 \
  --max-samples 20

  CUDA_VISIBLE_DEVICES=0 python eval_gqa_official.py \
  --model-path /perception-hl/zhuofan.xia/vlm_exps/textdat/tdat-7b-l0d32-s12g8z3 \
  --data-path /perception-hl/zhuofan.xia/data/gqa/val_balanced_questions.json \
  --image-folder /perception-hl/zhuofan.xia/data/gqa/images \
  --output-dir ./gqa_debug_20 \
  --conv-mode vicuna_v1 \
  --temperature 0 \
  --max-new-tokens 8 \
  --max-samples 20 \
  --brief-instruction "Answer briefly with one or two words only."







MODEL_PATH=${MODEL_PATH:-/home/zhuofan.xia/gsva_pretrains/llava-v1_5-7b} \
CUDA_VISIBLE_DEVICES=0 python eval_gqa_official.py \
  --model-path "$MODEL_PATH" \
  --data-path /perception-hl/zhuofan.xia/data/gqa/val_balanced_questions.json \
  --image-folder /perception-hl/zhuofan.xia/data/gqa/images \
  --output-dir ./gqa_debug_20 \
  --conv-mode llava_v1 \
  --temperature 0 \
  --max-new-tokens 16 \
  --max-samples 20