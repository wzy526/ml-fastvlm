{
  "model_config": "LLaVA_CLIP_ViT_L_14_336px",
  "vision_encoder": "clip",
  "llm": "Llama",
  "input_resolution": "336x336",
  "visual_tokens": 576,
  "model_size": "0.5B",
  "stage": 2,
  "model_path": "./checkpoints/llava-v1.5-7b",
  "data_path": "/root/gqa_opendatalab/testdev_balanced_questions.json",
  "warmup_samples": 100,
  "total_samples": 0,
  "accumulated_latency_ms": 0.0,
  "avg_ttft_ms": 0.0,
  "calculation_method": "dist.all_reduce(sum)",
  "world_size": 1,
  "timestamp": "2025-08-14 21:42:04"
}