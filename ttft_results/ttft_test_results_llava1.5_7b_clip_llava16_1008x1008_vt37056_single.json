{
  "model_config": "LLaVA_CLIP_ViT_L_14_336px_LLaVA16",
  "vision_encoder": "clip_llava16",
  "llm": "Llama",
  "input_resolution": "1008x1008",
  "visual_tokens": 37056,
  "model_size": "0.5B",
  "stage": 2,
  "model_path": "./checkpoints/llava-v1.5-7b",
  "data_path": "/root/gqa_opendatalab/testdev_balanced_questions.json",
  "warmup_samples": 100,
  "total_samples": 12478,
  "accumulated_latency_ms": 11448280.0,
  "avg_ttft_ms": 917.4771598012502,
  "calculation_method": "dist.all_reduce(sum)",
  "world_size": 1,
  "timestamp": "2025-08-16 06:55:21"
}